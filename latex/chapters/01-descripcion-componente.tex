\chapter{DESCRIPCIÓN DEL COMPONENTE}

\section{Descripción general del componente}

Este proyecto se compone de cuatro componentes interrelacionados. El Componente B constituye el motor que traduce la evidencia de interacción del estudiante en decisiones pedagógicas accionables. Su función principal es calcular el nivel de habilidad del estudiante, identificar qué conocimientos domina con suficiente confianza y determinar qué actividad o ítem presentar a continuación, incluyendo la dificultad apropiada y el tipo de soporte necesario. Los objetivos del componente son duales: por un lado, medir con precisión el desempeño del estudiante; por otro, favorecer el aprendizaje mediante trayectorias personalizadas, haciendo uso de la retroalimentación constante que mantiene con el Componente A, responsable de generar las actividades y las clases. Este enfoque se alinea con la lógica del aprendizaje adaptativo que predomina en la actualidad, basada en realizar ajustes del proceso de aprendizaje a partir de datos individuales en lugar de seguir rutas fijas predeterminadas \cite{ref1,ref2,ref3}.

\section{Objetivo general}

Desarrollar un motor adaptativo operativo que traduzca la evidencia de interacción del estudiante en decisiones pedagógicas fundamentadas, mediante la combinación de modelos psicométricos extensivamente validados (IRT y BKT/KT) para medir con precisión el desempeño del estudiante y construir trayectorias de aprendizaje genuinamente personalizadas que se ajusten dinámicamente a partir de datos individuales.

\section{Objetivos específicos}

\subsection{Implementar un sistema de medición dual del perfil del estudiante mediante señales complementarias}

Desarrollar un sistema que recoja eventos de interacción y calcule dos señales complementarias: un nivel continuo de habilidad $\theta$ basado en IRT para ordenar ítems informativos y reducir el error estándar de medición, y una probabilidad de dominio por habilidad fundamentada en BKT/KT para guiar la práctica espaciada y el refuerzo cuando el objetivo sea consolidar conocimientos de forma sostenida.

\subsection{Desarrollar una política de selección adaptativa basada en IRT para contextos de diagnóstico y certificación}

Implementar una política de selección que escoja el ítem que maximiza la información en torno al $\theta$ estimado, reduciendo rápidamente el error estándar y permitiendo alcanzar precisión localizada donde más importa, incorporando reglas de detención, restricciones de contenido curricular y limitaciones de exposición siguiendo las buenas prácticas establecidas en IRT y CAT.

\subsection{Implementar un sistema de rastreo de conocimiento basado en BKT/KT para orientar la práctica guiada}

Desarrollar mediante BKT/KT un sistema que mantenga una probabilidad de dominio por habilidad y la actualice tras cada interacción, modelando fenómenos como la adivinación y el desliz, de modo que la selección de la actividad subsiguiente se decida según el beneficio esperado: confirmar dominio incipiente, reducir incertidumbre o fomentar el aprendizaje en la zona de desarrollo más productiva.

\subsection{Diseñar una arquitectura fundamentada en el patrón MAPE-K para sistemas auto-adaptativos}

Implementar el Componente B como un bucle MAPE-K que monitorice respuestas y patrones de desempeño, analice el perfil del estudiante, planifique la siguiente actividad especificando ítem, dificultad y tipo de apoyo, y ejecute enviando recomendaciones explícitas al Componente A, garantizando trazabilidad, explicabilidad y la posibilidad de integrar organización semántica del contenido mediante grafos o rutas de aprendizaje.

\subsection{Establecer un contrato explícito de integración entre componentes que garantice trazabilidad}

Diseñar la integración $A \leftrightarrow B$ como un contrato de datos simple y explícito donde el Componente A envíe datos del usuario, tema activo e historial de interacciones, y el Componente B responda con el ítem propuesto, dificultad objetivo, tipo de ayuda recomendada y justificación concisa de la decisión, haciendo las decisiones auditables y proporcionando al docente evidencia clara del proceso.

\subsection{Implementar salvaguardas de validez, equidad y mecanismos de monitoreo continuo del sistema}

Desarrollar salvaguardas que resguarden la validez y equidad mediante el reporte de precisión alcanzada, validación del ajuste del modelo, monitorización de exposición equilibrada de temas e ítems, reporte de ganancia pre-post, control de métricas predictivas como AUC o log-loss, y realización de pruebas DIF y análisis de brechas entre perfiles para detectar posibles sesgos.

\section{Alcance del componente}

El alcance del Componente B comprende el desarrollo de un motor adaptativo operativo con las siguientes características funcionales y técnicas:

\begin{enumerate}[label=\roman*.]
  \item \textbf{Integración de modelos complementarios:} Combinar IRT para diagnóstico o evaluación sumativa con BKT/KT para guiar la práctica continua, aprovechando las fortalezas de ambos enfoques para ofrecer una evaluación integral que mida y fomente el aprendizaje.
  \item \textbf{Orquestación de la selección de ítems:} Orquestar la selección de ítems mediante reglas claras de parada, cobertura curricular y exposición equilibrada que permitan determinar cuándo la evaluación ha alcanzado suficiente precisión, garantizando que todos los temas relevantes sean cubiertos y evitando la sobreutilización de ítems específicos.
  \item \textbf{Panel de métricas para validación docente:} Publicar un panel de métricas (precisión diagnóstica, eficiencia, progreso del estudiante, calidad predictiva y equidad) accesible para que los docentes validen el funcionamiento del sistema, facilitando la transparencia y permitiendo intervenciones informadas cuando resulte necesario.
  \item \textbf{Gobernanza de datos y privacidad:} Documentar la gobernanza de datos y privacidad, especificando qué se registra, para qué propósito y cómo se protege la información, asegurando el cumplimiento de estándares éticos y regulatorios en el manejo de datos educativos sensibles.
\end{enumerate}

Todo ello integrado con el Componente A de manera que cada estudiante reciba un reto apropiado, en el momento oportuno, con las explicaciones y los apoyos adecuados a su nivel y necesidades específicas, logrando una experiencia de aprendizaje genuinamente personalizada y fundamentada en evidencia \cite{ref1,ref3,ref7,ref8,ref10}.

\section{Marco teórico}

\subsection{Aprendizaje adaptativo}

El aprendizaje adaptativo es una forma de enseñanza que se ajusta a cada estudiante en tiempo real. En vez de proponer la misma ruta para todos, el sistema observa evidencias (aciertos, errores, tiempo de respuesta, interacciones) y decide qué contenido, qué nivel de dificultad y qué apoyo conviene a continuación. Así, la progresión deja de ser lineal y se vuelve personalizada, manteniendo el foco en el dominio gradual de objetivos. Esta idea se formaliza y se sostiene en la literatura reciente sobre evaluación, personalización y uso responsable de IA en educación  \cite{ref1,ref3}.

Conviene distinguir lo adaptativo de lo simplemente ''personalizado''. La personalización puede implicar variedad de actividades o estilos, pero no siempre supone que el sistema mida y ajuste continuamente con base en datos. Lo adaptativo, en cambio, depende de un ciclo continuo de diagnóstico-–retroalimentación–-reajuste, y se apoya en un buen diseño instruccional: objetivos claros, progresiones definidas y evidencias útiles para decidir los siguientes pasos \cite{ref3}.

En la práctica, el ciclo luce así: (1) un breve diagnóstico, (2) selección de recursos y tareas ajustadas al nivel detectado, (3) retroalimentación oportuna, (4) una nueva medición que confirma avances o sugiere refuerzos y (5) ajustes de la ruta. La clave no es aumentar la cantidad de ejercicios, sino ofrecer los adecuados en el momento preciso. Este principio didáctico se alinea con marcos como los de Reigeluth y los primeros principios de Merrill, que recomiendan activar saberes previos, demostrar, aplicar e integrar lo aprendido \cite{ref3}.

La IA generativa ha aportado un motor útil para redactar explicaciones, proponer ejemplos y crear ejercicios alineados con la ruta de cada estudiante. Sin embargo, la evidencia disponible también advierte que estas herramientas no reemplazan la pedagogía ni la evaluación rigurosa; su valor aumenta cuando operan bajo criterios claros de calidad, ética y supervisión docente \cite{ref1}.

Un ejemplo concreto es PathRAG, que organiza el conocimiento como un grafo (conceptos y relaciones) para trazar caminos pertinentes según el perfil del estudiante. Estudios recientes en contextos universitarios híbridos reportan mejoras en participación, logro de competencias y percepción de inclusión cuando se integran rutas personalizadas con apoyo de IA generativa; aun así, subrayan límites metodológicos y la necesidad de diseños más robustos \cite{ref2}.

\subsection{Teoría de Respuesta al Ítem (IRT)}

La Teoría de Respuesta al Ítem (TRI o IRT) es una forma moderna de entender las pruebas: en lugar de mirar solo el puntaje total, analiza cómo responde una persona a cada ítem y, a partir de ello, estima su nivel en el rasgo que se quiere medir $\theta$. Con esa estimación, es posible seleccionar mejores preguntas, ubicar la dificultad donde más hace falta y conocer cuán precisa es la medición en cada tramo del continuo. Frente a la Teoría Clásica de Tests, su aporte central es la 'invariancia': medir con la misma escala, aunque cambien los sujetos o los ítems (dentro de ciertos supuestos)  \cite{ref1,ref2}.

\subsubsection{Ideas clave}

\begin{itemize}
  \item \textbf{Curva característica del ítem (CCI):} es un gráfico que muestra, para cada nivel de  $\theta$, la probabilidad de elegir la opción ''clave'' del ítem (por ejemplo, responder correctamente o indicar mayor rasgo). Su forma creciente refleja que, a mayor nivel del rasgo, mayor probabilidad de dar la respuesta asociada al rasgo \cite{ref1,ref2}.
  \item \textbf{Parámetros $a$, $b$ y $c$:} $a$ indica cuánto discrimina el ítem (qué tan bien separa a personas con niveles cercanos de  $\theta$), $b$ ubica la dificultad del ítem (el punto de la escala donde el ítem “decide”), y  $c$ modela el azar o pseudo-adivinación en ítems de opción correcta/incorrecta. No todos los modelos usan los tres: Rasch (1PL) usa solo b, 2PL usa a y b, y 3PL usa a, b, c \cite{ref1,ref2}.
  \item \textbf{Información del ítem/test:} indica dónde el ítem o el conjunto de ítems mide con mayor precisión. En IRT la precisión no es “plana”: puede ser excelente en un rango de $\theta$ y más baja en otros. Esto permite construir bancos de ítems que “cubran” la escala con precisión donde más importa \cite{ref2}.
\end{itemize}

\subsubsection{Supuestos relevantes}

\begin{itemize}
  \item \textbf{Unidimensionalidad:} los ítems de una escala deben reflejar esencialmente un solo rasgo dominante; si influyen varios rasgos a la vez, conviene usar modelos multidimensionales o depurar la escala \cite{ref2}.
  \item \textbf{Independencia local:} si ya sabemos el nivel de $\theta$, las respuestas a ítems distintos no deben depender entre sí. Cuando hay “pistas” entre ítems o se agrupan demasiado, este supuesto se rompe y la medición pierde calidad \cite{ref1,ref2}.
\end{itemize}

\subsubsection{Modelos más usados (visión práctica)}

\begin{itemize}
  \item \textbf{Ítems dicotómicos (correcto/incorrecto):} 1PL (Rasch), 2PL y 3PL. Rasch asume igual discriminación y sin azar; 2PL permite que la discriminación varíe; 3PL incluye el parámetro de pseudo-adivinación. Elegir el modelo depende del contexto y los datos \cite{ref1,ref2}.
  \item \textbf{Ítems politómicos (escalas Likert):} Modelos como Respuesta Graduada (Samejima) o Crédito Parcial. En el Modelo de Respuesta Graduada, cada salto entre categorías tiene un “umbral” de dificultad, y un único parámetro $a$ de discriminación para el ítem. Esto es muy útil para cuestionarios con varias opciones de respuesta \cite{ref5}.
\end{itemize}

\subsubsection{Bancos de ítems y pruebas adaptativas (CAT)}

Al estimar $\theta$ en tiempo real y conocer la información de cada ítem, es posible elegir la siguiente pregunta que aporte máxima precisión justo alrededor del nivel estimado del estudiante. Así nacen los tests adaptativos: cada persona responde un conjunto distinto de preguntas, pero todos son evaluados en la misma escala. En tu proyecto, esto es clave para que el Componente B seleccione o recomiende ítems con mayor ''ganancia informativa'' \cite{ref4,ref5}.

\subsubsection{Integración con los Componentes A y B}

\begin{enumerate}[label=\arabic*.]
  \item (B) A partir de las respuestas del estudiante, se estima $\theta$ con un modelo IRT apropiado (2PL/3PL para ítems dicotómicos; Respuesta Graduada para Likert).
  \item (B) se consulta el banco de ítems para identificar cuáles ofrecen más información alrededor del $\theta$ actual (o del umbral de dominio). 
  \item (B$\rightarrow$A) se envía al Componente A la dificultad objetivo y, si aplica, los ítems recomendados o las pautas de complejidad. 
  \item (A) el Componente A genera la siguiente actividad con esa dificultad y pistas adecuadas. 
  \item (B) tras la actividad, se actualiza $\theta$ y se repite el ciclo. Este bucle mantiene rutas personalizadas y medibles  \cite{ref5,ref6}.
\end{enumerate}

\subsection{Sistemas de Tutoría Inteligente (ITS)}

Un Sistema de Tutoría Inteligente (ITS) es un software que intenta “parecerse” a una tutoría humana: observa cómo aprende el estudiante, le ofrece explicaciones y actividades a la medida, y retroalimenta en los momentos clave. La idea no es reemplazar al docente, sino multiplicar su apoyo para que cada persona avance a su propio ritmo y con la ayuda justa. Las revisiones recientes muestran que, bien implementados, los ITS mejoran el rendimiento y la participación, personalizan contenidos y apoyan la autorregulación del aprendizaje \cite{ref7}.

\subsubsection{Componentes típicos}

\begin{itemize}
  \item \textbf{Modelo del estudiante:} mantiene un ''perfil vivo'' con aciertos, errores, tiempos y progreso. 
  \item \textbf{Modelo del tutor:} decide qué explicar, qué actividad proponer y qué pista dar. 
  \item \textbf{Modelo de dominio:} representa el conocimiento de la materia (conceptos, habilidades, reglas).
  \item \textbf{Interfaz:} es la cara del sistema (pantallas, ejercicios, feedback). 
\end{itemize}

Con estos componentes, el ITS puede ajustar dificultad, secuencias y apoyos en tiempo real \cite{ref8}.

\subsubsection{Integración con los Componentes A y B}

\begin{enumerate}[label=\arabic*.]
  \item (B) observa respuestas, estima el nivel del estudiante en los temas clave y detecta dificultades.
  \item (B$\rightarrow$A) Envía al Componente A la dificultad recomendada, objetivos prioritarios y el tipo de intervención.
  \item (A) El Componente A genera la actividad/clase con esa dificultad y apoyo. 
  \item (B) tras la actividad, el ITS vuelve a medir y ajusta la ruta. Este bucle mantiene trayectorias personalizadas y medibles a lo largo del curso \cite{ref7,ref8}.
\end{enumerate}

\subsection{Algoritmos de selección adaptativa}

Seleccionar ''lo siguiente'' no es al azar: es decidir, con evidencia, cuál actividad o ítem conviene presentar para medir mejor o para ayudar a aprender mejor. En este proyecto, esa decisión vive en el Componente B (evaluación) y retroalimenta al Componente A (generación de clases). A grandes rasgos, hay dos familias bien establecidas: selección guiada por IRT (cuando buscamos medir con precisión) y selección guiada por BKT/KT (cuando buscamos acompañar la adquisición de habilidades en el tiempo).

\subsubsection{Selección guiada por IRT (medición precisa)}

La Teoría de Respuesta al Ítem (IRT) modela la probabilidad de respuesta correcta según el nivel del rasgo latente $\theta$ y los parámetros del ítem. Con esa base, la selección adaptativa típica elige el siguiente ítem con más información alrededor del $\theta$ estimado del estudiante. Esto reduce el error estándar de medición con menos preguntas y mantiene la dificultad ''justo donde más informa''. En la práctica, se inicia con un $\theta$ neutro o con un breve arranque \textit{warm-up}; y tras cada respuesta se reestima $\theta$ y se elige el ítem que maximiza la información (o criterios cercanos como la información de Fisher o la divergencia KL). Para mantener validez y equidad, se aplican restricciones de contenido (temas/objetivos), control de exposición (evitar sobre‑uso de ciertos ítems) y límites de longitud o precisión objetivo. En escalas politómicas (tipo Likert), la lógica es análoga: cada categoría aporta información en zonas distintas de la escala, y la selección prioriza donde la precisión es más útil  \cite{ref4,ref5}.

Qué aporta al Componente A$\leftrightarrow$B cuando el objetivo es certificar dominio o ubicar con exactitud el nivel, IRT permite pedir menos y medir mejor. El Componente B devuelve a A el rango de dificultad recomendada (y la cobertura temática pendiente), de modo que A genere actividades acordes a ese nivel y no ''sobre‑-o‑-subestime'' la exigencia \cite{ref4,ref5}.

\subsubsection{Selección guiada por BKT/KT (apoyo al aprendizaje)}

Bayesian Knowledge Tracing (BKT) sigue, para cada habilidad, la probabilidad de dominio del estudiante a lo largo del tiempo: considera un estado ''domina / no domina'' y cuatro parámetros intuitivos (conocimiento inicial, probabilidad de aprender tras una práctica, adivinación y desliz). Con ese perfil, el sistema decide el siguiente ejercicio según el mayor beneficio esperado: reducir la incertidumbre, confirmar dominio o provocar aprendizaje en la zona adecuada. En contextos reales se combinan además prerequisitos, espaciado para combatir el olvido y señales de compromiso (tiempos, rachas). La evidencia reciente muestra que BKT es eficaz para personalizar secuencias y mejorar resultados cuando la meta es progresar en habilidades específicas y no sólo ''medir una vez con precisión'' \cite{ref9,ref11}.

El aporte dentro del flujo entre los Componentes A$\leftrightarrow$B, BKT entrega al Componente A no sólo una dificultad recomendada, sino también la habilidad prioritaria, el tipo de apoyo (pista, ejemplo guiado, práctica adicional) y el momento oportuno para espaciado o refuerzo. Tras la actividad de A, B actualiza las probabilidades de dominio y repite el ciclo  \cite{ref10,ref11}.

\subsubsection{Estrategia híbrida}

En etapas tempranas o con bancos pequeños, BKT tiende a funcionar mejor porque necesita menos calibración de ítems y entrega señales útiles para enseñar. Cuando el banco crece y se busca una estimación fina del nivel, IRT gana relevancia: permite fijar una precisión objetivo y optimizar la ruta de ítems. Una política práctica es: usar BKT para guiar la práctica diaria (progreso por habilidades) y activar selección IRT en cortes de evaluación (diagnósticos o certificaciones). En la arquitectura del proyecto, esto se implementa como un bucle MAPE‑K: Monitorizar (respuestas), Analizar (IRT/BKT), Planificar (siguiente ítem o actividad) y Ejecutar (enviar a A), con conocimiento compartido del perfil del estudiante y del banco de ítems \cite{ref10}.

\subsection{Métricas de desempeño en sistemas adaptativos}

Las métricas no son un listado de números: son la forma en que demostramos que el sistema realmente ayuda a aprender y que lo hace de manera eficiente y justa. En un entorno adaptativo, medir implica dos planos que se retroalimentan: la calidad de la medición (¿qué tan bien estimamos el nivel del estudiante?) y la calidad de la enseñanza (¿qué tanto aprende y con qué esfuerzo?). A continuación se presentan métricas nucleares, escritas en lenguaje claro, conectando la literatura de pruebas adaptativas y trazado del conocimiento \cite{ref4, ref12}.

\subsubsection{Precisión y validez de medición (IRT/CAT)}

En pruebas adaptativas orientadas a medir con exactitud, la precisión se observa en el error estándar del estimador de habilidad,  $SE(\hat{\theta})$, y en la información del test alrededor del nivel estimado. Un buen algoritmo reduce  $SE(\hat{\theta})$ con menos ítems: ese equilibrio entre precisión y longitud del test es central. Para comparar métodos, es útil fijar la eficiencia (mismo número medio de ítems) y contrastar la precisión resultante, o al revés. En estudios recientes se equiparan ambos métodos (por ejemplo, IRT‑CAT vs. enfoques alternativos) y se evalúa la diferencia media y la correlación de los puntajes con respecto a un ''test completo'' considerado referencia  \cite{ref12}.

Otro indicador clave es la calibración/ajuste del modelo: el sesgo (diferencia promedio entre el estimado y el valor de referencia), la RMSE (raíz del error cuadrático medio) y las curvas de calibración por tramos de la escala. Para motores más flexibles, se recurre a medidas de divergencia como $KL({\pi} || {\pi})$, que cuantifican la pérdida de información entre la densidad verdadera de puntajes y la estimada; valores pequeños señalan mejor ajuste. En simulaciones de calibración y selección de modelo, la elección del criterio de información (p. ej., BIC) afecta de forma tangible la precisión final de las estimaciones \cite{ref12}.

\subsubsection{Eficiencia y carga de respuesta}

La eficiencia refleja cuántos ítems o cuánto tiempo necesita el sistema para alcanzar una precisión aceptable. Métricas prácticas incluyen: número promedio de ítems administrados, desviación típica de ese número (variabilidad de carga entre estudiantes), tiempo por objetivo alcanzado y porcentaje de ítems no administrados (ahorro respecto del banco total). Un buen sistema es más corto sin sacrificar precisión. Además, conviene analizar cómo varía la carga según el nivel verdadero: algunos algoritmos exigen más ítems en los extremos de la escala, otros en el centro; reconocer ese patrón ayuda a planificar bancos y reglas de parada  \cite{ref12}.

\subsubsection{Aprendizaje y progreso}

Cuando el objetivo es que el estudiante aprenda (no solo medir), importan indicadores de progreso: la ganancia entre pre‑- y post‑-prueba normalizada por la dificultad, la tasa de dominio por unidad, el tiempo/ítems hasta alcanzar un umbral de dominio, y la retención tras un intervalo (espaciado). En sistemas con trazado del conocimiento (KT/BKT), puede reportarse la probabilidad de dominio por habilidad y su evolución, verificando que las decisiones (refuerzo, explicación, práctica guiada) incrementen esa probabilidad de forma sostenida  \cite{ref4,ref9}.

\subsubsection{Calidad predictiva de la política adaptativa}

Para validar que el sistema ''elige bien lo siguiente'', se evalúa su poder de predicción de respuestas y de dominio futuro. Métricas comunes son log‑-loss (pérdida de probabilidad), AUC/ROC para acierto de la próxima respuesta y exactitud o F1 en clasificación de dominio/no‑-dominio. En secuenciación adaptativa, también se puede medir el beneficio esperado o “regret” acumulado frente a una política de referencia. Estas métricas no sustituyen a la evidencia de aprendizaje, pero aseguran que el motor de decisión es consistente y estable \cite{ref9,ref11}.

\subsubsection{Equidad y robustez}

Un sistema adaptativo debe ser justo y estable. La equidad se estudia con análisis DIF (ítems que favorecen a subgrupos), comparaciones de error/precisión y tasa de dominio entre perfiles, y auditorías de exposición de ítems. La robustez exige pruebas de sensibilidad a supuestos del modelo (unidimensionalidad, independencia local) y validación cruzada cuando se recalibra el banco. Por último, la transparencia en el uso de datos y la interpretabilidad de reportes para docentes son métricas de calidad percibida y confianza  \cite{ref5,ref12}.

\subsubsection{Reporte para la integración A$\leftrightarrow$B}

Para cerrar el ciclo A$\leftrightarrow$B, el Componente B debe devolver un panel compacto: (i) precisión alcanzada ($SE(\hat{\theta})$,o intervalo de confianza de puntaje/total), (ii) eficiencia (ítems/tiempo vs. objetivo), (iii) progreso por habilidad (probabilidad de dominio y ganancia), (iv) calidad de predicción (AUC/log‑loss) y (v) equidad (DIF y exposición balanceada). Con ese resumen, el Componente A puede ajustar dificultad, apoyo y espaciado con criterio.
